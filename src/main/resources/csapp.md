#### 程序访问的局部性原理包括时间局部性和空间局部性。

**时间局部性**   指在最近的未来要用的信息很可能是现在正在使用的信息，（因为程序中存在循环）

**空间局部性**   是指在最近的未来要用到的信息，很可能与现在正在使用的信息在存储空间上是邻近的，因为指令通常是顺序存放，顺序执行的。

**高速缓冲技术**   利用程序访问的局部性原理，把程序中正在使用的部分存放在一个高速的容量较小的Cache中，使CPU的访存操作大多数针对Cache进行，从而大大提高程序的执行速度。

源文件------编译系统------可执行文件:xxx.java->xxx.javac  文本文件-->二进制文件

不同语言的编译器编译后产生的输出文件都是一样的汇编语言。

编译后成为汇编语言，汇编后成为机器语言

汇编系统:预处理(处理头文件并合并到本文件中)-编译-汇编-链接(把其他函数的代码链接到本文件中)

![](images\存储器层次结构.png)

文件是对IO设备的抽象，虚拟内存是对IO设备和主存的抽象，进程是对IO设备、主存、处理器的抽象。

> 内存的每个字节都由一个唯一的数字来标识，称为他的地址（虚拟地址）。对于一个字长为w位的机器来说，虚拟地址的范围为0~2^w-1，程序最多访问2^w个字节。小端法：对象字节顺序存储，大端法：对象字节倒序存储。

### 第三章：

gcc编译器编译出的汇编代码的顺序与源代码差距很大，这是为了优化性能。虽然C语言提供了一种模型，可以在内存中声明和分配各种数据类型的对象，但是机器代码将内存看成一个很大的、按字节寻址的数组，称为虚拟地址。C语言中的聚合数据类型例如数组和结构，在机器代码中用一组连续的字节来表示。

在目前的实现中，X86-64的内存的虚拟地址是由64位的字来表示的，但这些地址的高16位必须设置为0，所以一个地址实际上能够指定的是2^48或64TB范围内的一个字节，操作系统负责管理虚拟地址空间，将虚拟地址翻译成实际处理器内存中的物理地址。

操作数只能操作寄存器或者内存里的内容。所以pc寻址只能从内存或寄存器寻。

高级语言汇编后的汇编语言可以在任何机器上运行，直接写的汇编语言与机器强相关，在其他机器上可能不能运行。

不同架构的指令集处理器运行的机器语言也不一样。X86值的是Intel系列的处理器，X86-64指64位位长，AMD的处理器与Intel的指令集架构一模一样，都能运行同样的机器级程序。

汇编代码表示非常接近于机器级语言，但它提供了更好的阅读性

3.5

### 第九章：虚拟内存

虚拟内存是硬件异常、硬件地址翻译、主存、磁盘文件和内核软件的完美交互，它为每个进程提供了一个大的、一致的和私有的地址空间。

虚拟内存提供了三个重要的能力：

1. 它将主存看成是一个存储在磁盘上的地址空间的高速缓存，在主存中只保存活动区域，并根据需要在磁盘和主存之间来回传送数据，通过这种方式，它高速地使用了主存。
2. 它为每个进程提供了一致的地址空间，从而简化了内存管理。
3. 它保护了每个进程的地址空间不被其他进程破坏。

使用虚拟寻址，CPU通过生成一个虚拟地址来访问主存，这个虚拟地址在被送到内存之前先转换成适当的物理地址。将一个虚拟地址转换成物理地址的任务叫做地址翻译。

![](images\虚拟寻址.png)

##### 虚拟内存的基本思想：

地址空间的概念是很重要的，因为它清楚地区分了数据对象（字节）和它们的属性（地址）。一旦认识到了这种区别，那么我们就可以将其推广，允许每个数据对象有多个独立的地址，其中每个地址都选自一个不同的地址空间。主存中的每个字节都有一个选自VA的虚拟地址和PA的物理地址。

概念上而言，虚拟内存被组织为一个由存放在磁盘上的N个连续的字节大小的单元组成的数组(虚拟页)。每字节都有一个唯一的虚拟地址，作为到数组的索引。磁盘上数组的内容被缓存在主存中。

###### 页表：

> 页表存在于主存上，将虚拟页映射到物理页，每次MMU将一个VA翻译成PA时都会读取页表。OS负责维护页表的内容，以及在磁盘与DRAM之间来回传送页。

常驻内存的页表PTE存放了**已缓存到物理页PP（主存上）上的对应的虚拟页VP（磁盘上）有哪些和它们在PP上的位置**，以及**没有缓存到DRAM上的VP所对应的VA在哪**，当CPU引用VP3中的一个字时如果该字没有被缓存到页表上的话，称为缺页，VM系统会先选择页表中的一个牺牲页复制回磁盘并将VP3缓存到页表然后重新执行访问VP3的动作。

<img src="D:\images\微信图片_20210827115543.png" style="zoom:50%;" />

<img src="D:\images\微信图片_20210827115534.png" style="zoom:50%;" />

操作系统为每个进程提供了一个独立的页表，因而也就是一个独立的虚拟地址空间，虚拟内存为每个进程都提供了一致的地址空间，简化了内存的管理，所以说每个进程的VP都是一样的，但是映射到主存上的不同的位置。**注意，多个虚拟页面可以映射到同一个共享物理页面上。**

<img src="D:\images\QQ图片20210827142501.png" style="zoom:50%;" />

操作系统通过看每个进程的页表就能知道CPU想要访问的VP到底有没有缓存到PP上，以及PP上的具体位置并且访问。如果通过看页表发现VP没有缓存到PP上的话就会进行缺页拯救措施，选择一张牺牲页。

页面调度：将数据移进移出磁盘

### 第十章：系统级I/O

###### 内存模型

- 按照数据读取顺序和与 CPU 结合的紧密程度，CPU 缓存可以分为一级缓存，二级缓存，部分高端 CPU 还具有三级缓存。**每一级缓存中所储存的全部数据都是下一级缓存的一部分**，越靠近 CPU 的缓存越快也越小。所以 L1 缓存很小但很快(译注：L1 表示一级缓存)，并且紧靠着在使用它的 CPU 内核。L2 大一些，也慢一些，并且仍然只能被一个单独的 CPU 核使用。L3 在现代多核机器中更普遍，仍然更大，更慢，并且被单个插槽上的所有 CPU 核共享。最后，你拥有一块主存，由全部插槽上的所有 CPU 核共享。拥有三级缓存的的 CPU，到三级缓存时能够达到 95% 的命中率，只有不到 5% 的数据需要从内存中查询。

- ![](images\内存模型.png)

- 伪共享的非标准定义为：**缓存系统中是以缓存行（cache line）为单位存储的**，当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，就会无意中影响彼此的性能，这就是伪共享。

- *cpu 每次从主内存中获取数据的时候都会将相邻的数据存入到同一个缓存行中。假设我们访问一个Long内存对应的数组的时候，如果其中一个被加载到内存中，那么对应的后面的7个数据也会被加载到对应的缓存行中，这样就会非常快的访问数据*。

    - 缓存：位于CPU和主内存之间，有三层缓存，一个插槽内可以有多个cpu核，一个核独享一个寄存器和L1、L2高速缓存，插槽内的多个核共享L3高速缓存。同一插槽内的多个核访问L3内同一个缓存行中不同变量时，由于缓存一致性(MESI)原理，先到的线程会拿到数据X后会发出RFO(Request for owner)指令，该指令会发出请求通知其他线程该缓存行 为当下线程独自拥有，接下来的线程就算请求的是变量Y也无法在L3命中数据，只能去主内存中拿。
    - 表面上 X 和 Y 都是被独立线程操作的，而且两操作之间也没有任何关系。**只不过它们共享了一个缓存行，但所有竞争冲突都是来源于共享。**哪怕两个线程没有操作同一个变量，但是由于两个线程操作的两个变量是在同一个缓存行，也类似于两个线程访问了共享的数据。



###### 内存屏障(Memory Barrier)

> a) 保证特定操作的执行顺序 b) 影响某些数据的可见性
>
> 编译器和CPU能够重排序指令，保证最终相同的结果，并尝试优化性能。插入一条Memory Barrier会告诉CPU和编译器：在这条命令发生之前的必须在这条命令之前，在这条命令之后的必须待在命令之后。
>
> Memory Barrier（内存屏障）所做的另一件事是强制刷出各种 CPU cache（高速缓存）——比如，一个 Write-Barrier（写入屏障）将刷出所有在 Barrier 之前写入 cache 的数据，因此，任何尝试去读这些数据的线程将会读到它们的最新版本，而不管线程在哪个 CPU 核心或者哪个 CPU 插槽（Socket）上执行。

​		如果你的字段是 volatile 的，Java 内存模型（Java Memory Model）会在你写入字段之后插进一个 Write-Barrier 指令，并且在你读这个字段之前插入一个 Read-Barrier 指令。

这意味着，如果写入一个 volatile 字段，你知道这些事会发生：

1. 从写入字段这个时间点开始，任何线程访问该字段都会拿到更新后的数据。

2. 在写入字段之前的操作都确实执行过了，并且它们更新的数据也是可见的，

   因为 Memory Barrier 会刷出 cache 中所有先前的写入。（线程可能在缓存中对变量进行了写操作，但是还没有更新到主内存或L3中，其他线程从主内存中或者L3中读取到的该变量可能就不是最新的值）

内存屏障（Memory Barrier）是一个 CPU 指令，它允许你对数据在什么时候被其他进程可见作出确定的假设。在 Java 中，你可以用 volatile 关键字来实现它们。每次读写“volatile”字段都是一次相对开销较大的操作,内存屏障的确有一些开销 —— 编译器/CPU 不能重排序指令，会潜在的导致代码没有尽可能高效的利用 CPU，而且刷新 CPU cache 会对性能有明显的影响。

