#### CAP理论：

- 一致性C(consistency):所有节点访问同一份数据副本。

- 可用性A(Availability):非故障的节点在合理的时间返回合理的响应（不是错误或者超时的响应）。

- 分区容错性P(Partition tolerance):系统出现网络分区的时候，仍然能够对外提供服务。

> 网络分区：分布式系统中，多个节点之间的网络本来是连通的，如果因为某些故障（比如部分节点网络出现问题），某些节点之间不能连通，整个网络就分为了几块区域。

当要保证分区容错性的时候（分区容错性也是必须要保证的前提，不然分布式系统就没了意义），C和A只能保证一个，这两个特性之间是矛盾的。如果要实现C，故障节点出了问题不能读写，那么其他正常节点不能写保持数据一致性，这就和A发生了冲突（写请求不能响应）；如果要实现A，正常节点要能正常响应读写的话，故障节点由于没办法写，这就和C发生了冲突（数据不一致）。

如果网络分区正常的话（系统在绝大多数时候所处的状态），在不需要保证P的前提下，C和A是能够同时保证的。

#### CAP实际应用：

注册中心：

- zookeeper：保证了CP，即任何时刻请求系统都能得到相同的数据，但是zookeeper不保证每次请求的可用性，如在进行leader选举或者半数以上机器不可用的时候集群是不对外提供服务的。
- Eureka：保证了AP，在Eureka中不存在leader节点，每个节点都是平等的一致的，因此eureka不会出现zookeeper的leader选举或者一半机器以上挂掉服务就是不可用的情况。eureka保证即使大多数节点挂掉但只要有一个节点可用就能对外提供服务，只是可能这个节点的数据不是最新的。
- nacos：支持CP也支持AP。

Kafka：保证了AP。

在计数（点赞、收藏）等对数据不是特别敏感的场景时，保证AP与最终一致性即可，允许存在数据的中间状态。

#### 总结：

如果系统发生网络分区，要考虑选择AP还是CP，如果系统没有发生网络分区，要思考如何保证CA。

### BASE理论：

- 基本可用(Basically Available):分布式系统出现不可预知错误的时候，**允许损失部分可用性**，但是系统整体还是能对外提供服务。
    - 响应时间上的损失：正常情况下处理用户请求需要0.5s，但是由于系统故障，返回时间变为3s。
    - 系统功能上的损失：正常情况下用户能访问系统的所有功能，由于系统访问量激增，部分非核心功能无法使用。
- 软状态(Soft-state):软状态指允许系统中的数据存在中间状态（CAP理论中的数据不一致）,并认为该中间状态的数据不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。
- 最终一致性(Eventually Consistent)：最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步之后最终能够达到一致的状态，因此最终一致性的本职是需要系统中保证最终数据达成一致，而不需要实时保证系统数据的强一致性。

### BASE理论核心思想：

即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。

> 也就是牺牲数据的一致性来满足系统的高可用，系统中一部分数据不可用或者不一致时，仍需要保持系统整体"主要可用".

BASE理论本质上是对CAP的延申和补充，更准确的说是对AP方案 的补充，AP方案只是在系统发生网络分区的时候放弃了一致性，但不是永远放弃一致性，在分区故障恢复后，系统应该达到 最终一致性。

#### 分布式一致性的三种级别：

- 强一致性：系统写入什么读出来就是什么。
- 弱一致性：不一定能读到系统最新写入的值，也不确保在多长时间之后读到的数据是最新的，会尽量保证在某个时刻达到数据的一致性。
- 最终一致性：弱一致性的升级版，系统会保证在一定时间内达到数据的一致性。

#### 实现最终一致性的方案：

- 读时修复：在读取数据时，检测数据的不一致，进行修复。
- 写时修复：在写入数据时，检测数据的不一致，进行修复。
- 异步修复：最常用的方式，定时检测数据服务的一致性，并进行修复。

写时修复对性能的损耗最低。



### 分布式ID

- UUID：
    - 优点：生成方便，Java自身提供了工具；保证唯一性。
    - 缺点：太长，不适合做数据库主键；生成出的数据没有规则，对于有ID业务规则需要的场景不友好（订单：产生时间等）；无序，MySQL查询效率低下。
- MySQL自增（专门拿一台服务器做发号器）
    - 优点：实现简单；性能好
    - 缺点：每一次获取分布式id都需要查询数据库；作为单机数据库在高并发场景如果造成宕机就会很麻烦。
- MySQL集群(修改起始值和步长，一台id1 3 5 7...;一台id2 4  6 8...)
    - 优点：解决单机DB问题。
    - 缺点：不利于后续扩容，而且实际上单机数据库自身压力依然大。
- 基于数据库的号段模式
    - 从数据库批量的获取自增ID，每次从数据库取出一个号段范围，例如 (1,1000] 代表1000个ID，具体的业务服务将本号段生成1~1000的自增ID并加载到内存。
- 雪花算法



###  Mysql 高可用

#### 主从复制

优点：读写分离，负载均衡

> 主负责写，从负责读，从库不断从主库同步数据，保持数据一致。从库还可以水平扩展以应对不断增加的读请求。

建立主从集群，一主一从，一主多从。

同步数据的过程：主先开启数据写入binlog模式，从请求获取主的binlog，并将日志写进relylog中，再起一个SQL线程将relylog中数据解析成sql写进库中。



#### 分库分表

> 读请求增加时通过水平拓展从服务器提升性能。但是写请求增加时水平拓展master是解决不了问题的，因为数据要保持一致性，写操作要在两台master之间同步，反而重复了。

当数据库表数据太大时需考虑分库分表。分库分表之后还要考虑分布式id问题。

### 集群主从读写关系
#### MySQL
主写从读
#### Redis
主读写，从只读
#### Kafka

#### zookeeper
主写，从、观察者读；写请求如果发到了从节点上，从节点会将该命令转发给leader处理，leader处理完毕之后再分发给从


客户端写入流程说明：
1. client向zk中的server发送写请求，如果该server不是leader，则会将该写请求转发给leader server，leader将请求事务以proposal形式分发给follower；
2. 当follower收到收到leader的proposal时，根据接收的先后顺序处理proposal；
3. 当Leader收到follower针对某个proposal过半的ack后，则发起事务提交，重新发起一个commit的proposal
4. Follower收到commit的proposal后，记录事务提交，并把数据更新到内存数据库；
5. 当写成功后，反馈给client。

**服务节点初始化同步：**
在集群运行过程当中如果有一个follower节点宕机，由于宕机节点没过半，集群仍然能正常服务。当leader 收到新的客户端请求，此时无法同步给宕机的节点。造成数据不一至。为了解决这个问题，当节点启动时，第一件事情就是找当前的Leader，比对数据是否一至。不一至则开始同步,同步完成之后在进行对外提供服务。
如何比对Leader的数据版本呢，这里通过ZXID事物ID来确认。比Leader小就需要同步。
**ZXID说明：**投票机制说明
ZXID是一个长度64位的数字，其中低32位是按照数字递增，任何数据的变更都会导致,低32位的数字简单加1。高32位是leader周期编号，每当选举出一个新的leader时，新的leader就从本地事物日志中取出ZXID,然后解析出高32位的周期编号，进行加1，再将低32位的全部设置为0。这样就保证了每次新选举的leader后，保证了ZXID的唯一性而且是保证递增的。 
